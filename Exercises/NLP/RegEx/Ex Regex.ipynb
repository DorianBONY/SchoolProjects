{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84cba7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If\t SCONJ \n",
      "you\t PRON \n",
      "do\t AUX \n",
      "n’t\t PART \n",
      "know\t VERB \n",
      "where\t SCONJ \n",
      "you\t PRON \n",
      "are\t AUX \n",
      "going\t VERB \n",
      "any\t DET \n",
      "road\t NOUN \n",
      "can\t AUX \n",
      "take\t VERB \n",
      "you\t PRON \n",
      "there\t ADV \n",
      ".\t PUNCT \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"If you don’t know where you are going any road can take you there.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text}\\t {token.pos_} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abff3e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grace VERB\n",
      "me PRON\n",
      "no DET\n",
      "grace NOUN\n",
      ", PUNCT\n",
      "nor CCONJ\n",
      "uncle VERB\n",
      "me PRON\n",
      "no DET\n",
      "uncle NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Grace me no grace, nor uncle me no uncle\")\n",
    "for t in doc: print(t, t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7dbbe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Grace', 'NOUN'),\n",
       " Ellipsis,\n",
       " ('grace', 'NOUN'),\n",
       " Ellipsis,\n",
       " ('uncle', 'ADP'),\n",
       " Ellipsis,\n",
       " ('uncle', 'NOUN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "text = nltk.word_tokenize(\"Grace me no grace, nor uncle me no uncle\")\n",
    "nltk.pos_tag(text,tagset='universal')\n",
    "[('Grace', 'NOUN'), ..., ('grace', 'NOUN'), ..., ('uncle', 'ADP'), ..., ('uncle', 'NOUN')]\n",
    "# ADP here is an Adposition (it's complicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4607721",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6eb341f44582>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpersons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'PERSON'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# and list the 12 most common ones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpersons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "r = requests.get('http://www.gutenberg.org/files/11/11-0.txt')\n",
    "doc = nlp(r.text.split(\"*** END\")[0])\n",
    "# collect all the entities that are tagged PERSON\n",
    "persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "# and list the 12 most common ones\n",
    "Counter(persons).most_common(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd11735",
   "metadata": {},
   "outputs": [],
   "source": [
    "rabbit_ner = [(ent.text, ent.label_) for ent in doc.ents if \"Rabbit\" in ent.text]\n",
    "Counter(rabbit_ner).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cc096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 3.918s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 1.063s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.960s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.651s.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0ce12ef8be2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m \u001b[0mtfidf_feature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m plot_top_words(\n\u001b[0;32m     82\u001b[0m     \u001b[0mnmf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Topics in NMF model (Frobenius norm)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data, _ = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "data_samples = data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\n",
    "    \"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "    \"n_samples=%d and n_features=%d...\" % (n_samples, n_features)\n",
    ")\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=0.1, l1_ratio=0.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\n",
    ")\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
    "    \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "t0 = time()\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1,\n",
    "    beta_loss=\"kullback-leibler\",\n",
    "    solver=\"mu\",\n",
    "    max_iter=1000,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    ").fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf,\n",
    "    tfidf_feature_names,\n",
    "    n_top_words,\n",
    "    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa463c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
